{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 7 - R√©alisez une analyse de sentiments gr√¢ce au Deep Learning\n",
    "\n",
    "> üéì OpenClassrooms ‚Ä¢ Parcours [AI Engineer](https://openclassrooms.com/fr/paths/795-ai-engineer) | üëã *√âtudiant* : [David Scanu](https://www.linkedin.com/in/davidscanu14/)\n",
    "\n",
    "## üìù Contexte\n",
    "\n",
    "Dans le cadre de ma formation d'AI Engineer chez OpenClassrooms, ce projet s'inscrit dans un sc√©nario professionnel o√π j'interviens en tant qu'ing√©nieur IA chez MIC (Marketing Intelligence Consulting), entreprise de conseil sp√©cialis√©e en marketing digital.\n",
    "\n",
    "Notre client, Air Paradis (compagnie a√©rienne), souhaite **anticiper les bad buzz sur les r√©seaux sociaux**. La mission consiste √† d√©velopper un produit IA permettant de **pr√©dire le sentiment associ√© √† un tweet**, afin d'am√©liorer la gestion de sa r√©putation en ligne.\n",
    "\n",
    "## ‚ö° Mission\n",
    "\n",
    "> D√©velopper un mod√®le d'IA permettant de pr√©dire le sentiment associ√© √† un tweet.\n",
    "\n",
    "Cr√©er un prototype fonctionnel d'un mod√®le d'**analyse de sentiments pour tweets** selon trois approches diff√©rentes :\n",
    "\n",
    "1. **Mod√®le sur mesure simple** : Approche classique (r√©gression logistique) pour une pr√©diction rapide\n",
    "2. **Mod√®le sur mesure avanc√©** : Utilisation de r√©seaux de neurones profonds avec diff√©rents word embeddings\n",
    "3. **Mod√®le avanc√© BERT** : Exploration de l'apport en performance d'un mod√®le BERT\n",
    "\n",
    "Cette mission implique √©galement la mise en ≈ìuvre d'une d√©marche MLOps compl√®te :\n",
    "- Utilisation de MLFlow pour le tracking des exp√©rimentations et le stockage des mod√®les\n",
    "- Cr√©ation d'un pipeline de d√©ploiement continu (Git + Github + plateforme Cloud)\n",
    "- Int√©gration de tests unitaires automatis√©s\n",
    "- Mise en place d'un suivi de performance en production via Azure Application Insight\n",
    "\n",
    "## üóìÔ∏è Plan de travail\n",
    "\n",
    "1. **Exploration et pr√©paration des donn√©es**\n",
    "   - Acquisition des donn√©es de tweets Open Source\n",
    "   - Analyse exploratoire et pr√©traitement des textes\n",
    "\n",
    "2. **D√©veloppement des mod√®les**\n",
    "   - Impl√©mentation du mod√®le classique (r√©gression logistique)\n",
    "   - Conception du mod√®le avanc√© avec diff√©rents word embeddings\n",
    "   - Test du mod√®le BERT pour l'analyse de sentiments\n",
    "   - Comparaison des performances via MLFlow\n",
    "\n",
    "3. **Mise en place de la d√©marche MLOps**\n",
    "   - Configuration de MLFlow pour le tracking des exp√©rimentations\n",
    "   - Cr√©ation du d√©p√¥t Git avec structure de projet appropri√©e\n",
    "   - Impl√©mentation des tests unitaires automatis√©s\n",
    "   - Configuration du pipeline de d√©ploiement continu\n",
    "\n",
    "4. **D√©ploiement et monitoring**\n",
    "   - D√©veloppement de l'API de pr√©diction avec FastAPI\n",
    "   - D√©ploiement sur Heroku\n",
    "   - Cr√©ation de l'interface de test (Streamlit ou Next.js)\n",
    "   - Configuration du suivi via Azure Application Insight\n",
    "\n",
    "5. **Communication**\n",
    "   - R√©daction de l'article de blog\n",
    "   - Pr√©paration du support de pr√©sentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/david/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/david/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import warnings\n",
    "import string\n",
    "\n",
    "# T√©l√©charger les ressources NLTK n√©cessaires\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Jeu de donn√©es : Sentiment140\n",
    "\n",
    "Le jeu de donn√©es [Sentiment140 dataset with 1.6 million tweets](https://www.kaggle.com/datasets/kazanova/sentiment140) est une ressource majeure pour l'analyse de sentiment sur Twitter, comprenant **1,6 million de tweets** extraits via l'API Twitter. Ces tweets ont √©t√© automatiquement annot√©s selon leur polarit√© sentimentale, offrant une base solide pour d√©velopper des mod√®les de classification de sentiment.\n",
    "\n",
    "Le jeu de donn√©es est organis√© en 6 colonnes distinctes :\n",
    "\n",
    "1. **target** : La polarit√© du sentiment exprim√© dans le tweet.\n",
    "   - 0 = sentiment n√©gatif\n",
    "   - 2 = sentiment neutre\n",
    "   - 4 = sentiment positif\n",
    "2. **ids** : L'identifiant unique du tweet (exemple : *2087*)\n",
    "3. **date** : La date et l'heure de publication du tweet.\n",
    "4. **flag** : La requ√™te utilis√©e pour obtenir le tweet.\n",
    "   - Exemple : *lyx*\n",
    "   - Si aucune requ√™te n'a √©t√© utilis√©e : *NO_QUERY*\n",
    "5. **user** : Le nom d'utilisateur de l'auteur du tweet.\n",
    "6. **text** : Le contenu textuel du tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 712 Œºs, sys: 37 Œºs, total: 749 Œºs\n",
      "Wall time: 754 Œºs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Define the URL and the local file path\n",
    "url = \"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip\"\n",
    "local_zip_path = \"./content/data/sentiment140.zip\"\n",
    "extract_path = \"./content/data\"\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    # Download the zip file\n",
    "    response = requests.get(url)\n",
    "    with open(local_zip_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Extract the contents of the zip file\n",
    "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "    # Delete the zip file\n",
    "    os.remove(local_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag             user                                               text\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = os.path.join(extract_path, 'training.1600000.processed.noemoticon.csv')\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "raw_data = pd.read_csv(csv_file_path, encoding='latin-1', names=column_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce dataframe contient 1600000 lignes et 6 colonnes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ce dataframe contient {raw_data.shape[0]} lignes et {raw_data.shape[1]} colonnes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommandations de mod√©lisation\n",
    "\n",
    "Approches recommand√©es pour l'analyse de sentiment:\n",
    "\n",
    "1. Approches classiques de Machine Learning:\n",
    "   - Mod√®les bas√©s sur les sacs de mots (BoW) ou TF-IDF avec classifieurs comme R√©gression Logistique, SVM, Random Forest ou Naive Bayes\n",
    "   - Avantages: rapides √† entra√Æner, interpr√©tables\n",
    "   - Inconv√©nients: ne capturent pas la s√©mantique et l'ordre des mots\n",
    "\n",
    "2. Word Embeddings + Classifieurs:\n",
    "   - Utiliser des embeddings pr√©-entra√Æn√©s (Word2Vec, GloVe, FastText) avec des classifieurs ML\n",
    "   - Avantages: capture la s√©mantique des mots, g√®re les mots hors vocabulaire (FastText)\n",
    "   - Inconv√©nients: perd l'information de s√©quence\n",
    "\n",
    "3. R√©seaux de neurones r√©currents (RNN, **LSTM**, GRU):\n",
    "   - Avantages: capture l'information s√©quentielle, efficace pour le texte\n",
    "   - Inconv√©nients: temps d'entra√Ænement plus long, risque de surapprentissage\n",
    "\n",
    "4. Mod√®les transformers (BERT, Sentence Transformers, RoBERTa, DistilBERT):\n",
    "   - Fine-tuning de mod√®les pr√©-entra√Æn√©s sp√©cifiques √† Twitter comme BERTweet\n",
    "   - Avantages: √©tat de l'art en NLP, capture le contexte bidirectionnel\n",
    "   - Inconv√©nients: co√ªteux en ressources, complexe √† d√©ployer\n",
    "\n",
    "5. Approches d'ensemble:\n",
    "   - Combiner plusieurs mod√®les pour obtenir de meilleures performances\n",
    "   - Avantages: souvent meilleures performances, plus robuste\n",
    "   - Inconv√©nients: complexit√© accrue, temps d'inf√©rence plus long\n",
    "\n",
    "Consid√©rations importantes:\n",
    "\n",
    "1. D√©s√©quilibre des classes: utiliser des techniques comme SMOTE, sous-√©chantillonnage, ou pond√©ration des classes\n",
    "2. Validation crois√©e: essentielle pour √©valuer correctement les performances\n",
    "3. M√©triques d'√©valuation: ne pas se limiter √† l'accuracy, utiliser F1-score, pr√©cision, rappel, et AUC-ROC\n",
    "4. Interpr√©tabilit√©: pour certaines applications, privil√©gier des mod√®les interpr√©tables ou utiliser SHAP/LIME\n",
    "5. D√©pendance temporelle: consid√©rer l'√©volution du langage sur Twitter au fil du temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes du mentor :**\n",
    "\n",
    "Voici le texte extrait de cette capture d'√©cran :\n",
    "\n",
    "- Cr√©ation de deux mod√®les de Deep Learning, dont au moins un avec un layer LSTM.\n",
    "- Simulation selon deux techniques de pr√©-traitement (lemmatization, stemming) sur l'un des 2 mod√®les, afin de choisir la technique pour la suite des simulations.\n",
    "- Simulation selon 2 approches de word embedding (parmi Word2VEc, Glove, FastText), entra√Æn√©s avec le jeu de donn√©es ou pr√©-entra√Æn√©s sur au moins un des 2 mod√®les de Deep Learning, afin de choisir l'embedding pour la suite des simulations.\n",
    "- Cr√©ation ensuite d'un mod√®le BERT, il y a 2 approches possibles :\n",
    "  - G√©n√©rer des features (sentence embedding) √† partir d'un TFBertModel (Hugging Face) ou d'un d'un model via le Hub TensorFlow, puis ajouter une ou des couches de classification\n",
    "  - Utiliser directement un mod√®le Hugging Face de type TFBertForSequenceClassification\n",
    "- En option tester USE (Universal Sentence Encoding) pour le feature engineering\n",
    "\n",
    "Probl√®mes et erreurs courants :\n",
    "- Temps de traitement et limitation de ressources en TensorFlow-Keras.\n",
    "- Inspirer des exemples de mod√®les cit√©s en ressources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approches classiques de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentiment_label(df):\n",
    "    df['target'] = df['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    return df\n",
    "\n",
    "df = convert_sentiment_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# Fonction de pr√©traitement pour les tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Pr√©traite un tweet en appliquant plusieurs transformations :\n",
    "    - Conversion en minuscules\n",
    "    - Remplacement des URLs, mentions et hashtags par des tokens sp√©ciaux\n",
    "    - Suppression des caract√®res sp√©ciaux\n",
    "    - Tokenisation et lemmatisation\n",
    "    - Suppression des stopwords\n",
    "    \"\"\"\n",
    "    # V√©rifier si le tweet est une cha√Æne de caract√®res\n",
    "    if not isinstance(tweet, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remplacer les URLs par un token sp√©cial\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', tweet)\n",
    "    \n",
    "    # Remplacer les mentions par un token sp√©cial\n",
    "    tweet = re.sub(r'@\\w+', '<MENTION>', tweet)\n",
    "    \n",
    "    # Traiter les hashtags (conserver le # comme token s√©par√© et le mot qui suit)\n",
    "    tweet = re.sub(r'#(\\w+)', r'# \\1', tweet)\n",
    "    \n",
    "    # Supprimer les caract√®res sp√©ciaux et les nombres, mais garder les tokens sp√©ciaux\n",
    "    tweet = re.sub(r'[^\\w\\s<>@#!?]', '', tweet)\n",
    "    \n",
    "    # Tokenisation\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Supprimer les stopwords, mais conserver les n√©gations importantes\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    important_words = {'no', 'not', 'nor', 'neither', 'never', 'nobody', 'none', 'nothing', 'nowhere'}\n",
    "    stop_words = stop_words - important_words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Rejoindre les tokens en une cha√Æne\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def process_in_parallel(df, func, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Applique une fonction √† un DataFrame en le divisant en parties et en traitant chaque partie en parall√®le.\n",
    "    Permet d'acc√©l√©rer le traitement sur les ordinateurs multi-coeurs.\n",
    "    \"\"\"\n",
    "    df_split = np.array_split(df, n_jobs)\n",
    "    pool = Pool(n_jobs)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def apply_preprocessing(df_part):\n",
    "    df_part['processed_text'] = df_part['text'].apply(preprocess_tweet)\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    50000\n",
       "1    50000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pr√©paration des donn√©es pour l'entra√Ænement\n",
    "\n",
    "def downsample_data(df, n_samples=50000):\n",
    "    \"\"\"\n",
    "    R√©duit la taille d'un DataFrame en √©chantillonnant al√©atoirement un nombre sp√©cifi√© de lignes pour chaque classe.\n",
    "    \"\"\"\n",
    "    negative_samples = df[df['target'] == 0].sample(n=n_samples, random_state=42)\n",
    "    positive_samples = df[df['target'] == 1].sample(n=n_samples, random_state=42)\n",
    "    return pd.concat([negative_samples, positive_samples])\n",
    "\n",
    "balanced_df = downsample_data(df, n_samples=50000)\n",
    "balanced_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr√©traitement des tweets en cours...\n",
      "Pr√©traitement termin√© !\n"
     ]
    }
   ],
   "source": [
    "# Appliquer le pr√©traitement √† tous les tweets\n",
    "print(\"Pr√©traitement des tweets en cours...\")\n",
    "# balanced_df['processed_text'] = balanced_df['text'].apply(preprocess_tweet)\n",
    "# Utilisation\n",
    "balanced_df = process_in_parallel(balanced_df, apply_preprocessing, n_jobs=8)\n",
    "print(\"Pr√©traitement termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212188</th>\n",
       "      <td>0</td>\n",
       "      <td>1974671194</td>\n",
       "      <td>Sat May 30 13:36:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>simba98</td>\n",
       "      <td>@xnausikaax oh no! where did u order from? tha...</td>\n",
       "      <td>&lt; MENTION &gt; oh no ! u order ? thats horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299036</th>\n",
       "      <td>0</td>\n",
       "      <td>1997882236</td>\n",
       "      <td>Mon Jun 01 17:37:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Seve76</td>\n",
       "      <td>A great hard training weekend is over.  a coup...</td>\n",
       "      <td>great hard training weekend couple day rest le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475978</th>\n",
       "      <td>0</td>\n",
       "      <td>2177756662</td>\n",
       "      <td>Mon Jun 15 06:39:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>x__claireyy__x</td>\n",
       "      <td>Right, off to work  Only 5 hours to go until I...</td>\n",
       "      <td>right work 5 hour go im free xd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588988</th>\n",
       "      <td>0</td>\n",
       "      <td>2216838047</td>\n",
       "      <td>Wed Jun 17 20:02:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Balasi</td>\n",
       "      <td>I am craving for japanese food</td>\n",
       "      <td>craving japanese food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138859</th>\n",
       "      <td>0</td>\n",
       "      <td>1880666283</td>\n",
       "      <td>Fri May 22 02:03:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>djrickdawson</td>\n",
       "      <td>Jean Michel Jarre concert tomorrow  gotta work...</td>\n",
       "      <td>jean michel jarre concert tomorrow got ta work...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target         ids                          date      flag            user                                               text                                     processed_text\n",
       "212188       0  1974671194  Sat May 30 13:36:31 PDT 2009  NO_QUERY         simba98  @xnausikaax oh no! where did u order from? tha...       < MENTION > oh no ! u order ? thats horrible\n",
       "299036       0  1997882236  Mon Jun 01 17:37:11 PDT 2009  NO_QUERY          Seve76  A great hard training weekend is over.  a coup...  great hard training weekend couple day rest le...\n",
       "475978       0  2177756662  Mon Jun 15 06:39:05 PDT 2009  NO_QUERY  x__claireyy__x  Right, off to work  Only 5 hours to go until I...                    right work 5 hour go im free xd\n",
       "588988       0  2216838047  Wed Jun 17 20:02:12 PDT 2009  NO_QUERY          Balasi                    I am craving for japanese food                               craving japanese food\n",
       "138859       0  1880666283  Fri May 22 02:03:31 PDT 2009  NO_QUERY    djrickdawson  Jean Michel Jarre concert tomorrow  gotta work...  jean michel jarre concert tomorrow got ta work..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entra√Ænement: 80000 exemples\n",
      "Taille de l'ensemble de test: 20000 exemples\n"
     ]
    }
   ],
   "source": [
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = balanced_df['processed_text']\n",
    "y = balanced_df['target']\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Taille de l'ensemble d'entra√Ænement: {X_train.shape[0]} exemples\")\n",
    "print(f\"Taille de l'ensemble de test: {X_test.shape[0]} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entra√Ænement: 80000 exemples\n",
      "Taille de l'ensemble de test: 20000 exemples\n",
      "Vectorisation termin√©e !\n"
     ]
    }
   ],
   "source": [
    "# Vectorisation des textes avec TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Cr√©er les vectoriseurs\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "bow_vectorizer = CountVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "# Transformer les textes en vecteurs TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Transformer les textes en vecteurs BoW\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Vectorisation termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
