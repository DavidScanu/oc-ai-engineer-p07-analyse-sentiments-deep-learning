{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 7 - R√©alisez une analyse de sentiments gr√¢ce au Deep Learning\n",
    "\n",
    "> üéì OpenClassrooms ‚Ä¢ Parcours [AI Engineer](https://openclassrooms.com/fr/paths/795-ai-engineer) | üëã *√âtudiant* : [David Scanu](https://www.linkedin.com/in/davidscanu14/)\n",
    "\n",
    "## üìù Contexte\n",
    "\n",
    "Dans le cadre de ma formation d'AI Engineer chez OpenClassrooms, ce projet s'inscrit dans un sc√©nario professionnel o√π j'interviens en tant qu'ing√©nieur IA chez MIC (Marketing Intelligence Consulting), entreprise de conseil sp√©cialis√©e en marketing digital.\n",
    "\n",
    "Notre client, Air Paradis (compagnie a√©rienne), souhaite **anticiper les bad buzz sur les r√©seaux sociaux**. La mission consiste √† d√©velopper un produit IA permettant de **pr√©dire le sentiment associ√© √† un tweet**, afin d'am√©liorer la gestion de sa r√©putation en ligne.\n",
    "\n",
    "## ‚ö° Mission\n",
    "\n",
    "> D√©velopper un mod√®le d'IA permettant de pr√©dire le sentiment associ√© √† un tweet.\n",
    "\n",
    "Cr√©er un prototype fonctionnel d'un mod√®le d'**analyse de sentiments pour tweets** selon trois approches diff√©rentes :\n",
    "\n",
    "1. **Mod√®le sur mesure simple** : Approche classique (r√©gression logistique) pour une pr√©diction rapide\n",
    "2. **Mod√®le sur mesure avanc√©** : Utilisation de r√©seaux de neurones profonds avec diff√©rents word embeddings\n",
    "3. **Mod√®le avanc√© BERT** : Exploration de l'apport en performance d'un mod√®le BERT\n",
    "\n",
    "Cette mission implique √©galement la mise en ≈ìuvre d'une d√©marche MLOps compl√®te :\n",
    "- Utilisation de MLFlow pour le tracking des exp√©rimentations et le stockage des mod√®les\n",
    "- Cr√©ation d'un pipeline de d√©ploiement continu (Git + Github + plateforme Cloud)\n",
    "- Int√©gration de tests unitaires automatis√©s\n",
    "- Mise en place d'un suivi de performance en production via Azure Application Insight\n",
    "\n",
    "## üóìÔ∏è Plan de travail\n",
    "\n",
    "1. **Exploration et pr√©paration des donn√©es**\n",
    "   - Acquisition des donn√©es de tweets Open Source\n",
    "   - Analyse exploratoire et pr√©traitement des textes\n",
    "\n",
    "2. **D√©veloppement des mod√®les**\n",
    "   - Impl√©mentation du mod√®le classique (r√©gression logistique)\n",
    "   - Conception du mod√®le avanc√© avec diff√©rents word embeddings\n",
    "   - Test du mod√®le BERT pour l'analyse de sentiments\n",
    "   - Comparaison des performances via MLFlow\n",
    "\n",
    "3. **Mise en place de la d√©marche MLOps**\n",
    "   - Configuration de MLFlow pour le tracking des exp√©rimentations\n",
    "   - Cr√©ation du d√©p√¥t Git avec structure de projet appropri√©e\n",
    "   - Impl√©mentation des tests unitaires automatis√©s\n",
    "   - Configuration du pipeline de d√©ploiement continu\n",
    "\n",
    "4. **D√©ploiement et monitoring**\n",
    "   - D√©veloppement de l'API de pr√©diction avec FastAPI\n",
    "   - D√©ploiement sur Heroku\n",
    "   - Cr√©ation de l'interface de test (Streamlit ou Next.js)\n",
    "   - Configuration du suivi via Azure Application Insight\n",
    "\n",
    "5. **Communication**\n",
    "   - R√©daction de l'article de blog\n",
    "   - Pr√©paration du support de pr√©sentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/david/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/david/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import warnings\n",
    "import string\n",
    "\n",
    "# T√©l√©charger les ressources NLTK n√©cessaires\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Jeu de donn√©es : Sentiment140\n",
    "\n",
    "Le jeu de donn√©es [Sentiment140 dataset with 1.6 million tweets](https://www.kaggle.com/datasets/kazanova/sentiment140) est une ressource majeure pour l'analyse de sentiment sur Twitter, comprenant **1,6 million de tweets** extraits via l'API Twitter. Ces tweets ont √©t√© automatiquement annot√©s selon leur polarit√© sentimentale, offrant une base solide pour d√©velopper des mod√®les de classification de sentiment.\n",
    "\n",
    "Le jeu de donn√©es est organis√© en 6 colonnes distinctes :\n",
    "\n",
    "1. **target** : La polarit√© du sentiment exprim√© dans le tweet.\n",
    "   - 0 = sentiment n√©gatif\n",
    "   - 2 = sentiment neutre\n",
    "   - 4 = sentiment positif\n",
    "2. **ids** : L'identifiant unique du tweet (exemple : *2087*)\n",
    "3. **date** : La date et l'heure de publication du tweet.\n",
    "4. **flag** : La requ√™te utilis√©e pour obtenir le tweet.\n",
    "   - Exemple : *lyx*\n",
    "   - Si aucune requ√™te n'a √©t√© utilis√©e : *NO_QUERY*\n",
    "5. **user** : Le nom d'utilisateur de l'auteur du tweet.\n",
    "6. **text** : Le contenu textuel du tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 712 Œºs, sys: 37 Œºs, total: 749 Œºs\n",
      "Wall time: 754 Œºs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Define the URL and the local file path\n",
    "url = \"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip\"\n",
    "local_zip_path = \"./content/data/sentiment140.zip\"\n",
    "extract_path = \"./content/data\"\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    # Download the zip file\n",
    "    response = requests.get(url)\n",
    "    with open(local_zip_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Extract the contents of the zip file\n",
    "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "    # Delete the zip file\n",
    "    os.remove(local_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag             user                                               text\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = os.path.join(extract_path, 'training.1600000.processed.noemoticon.csv')\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "raw_data = pd.read_csv(csv_file_path, encoding='latin-1', names=column_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce dataframe contient 1600000 lignes et 6 colonnes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ce dataframe contient {raw_data.shape[0]} lignes et {raw_data.shape[1]} colonnes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommandations de mod√©lisation\n",
    "\n",
    "Approches recommand√©es pour l'analyse de sentiment:\n",
    "\n",
    "1. Approches classiques de Machine Learning:\n",
    "   - Mod√®les bas√©s sur les sacs de mots (BoW) ou TF-IDF avec classifieurs comme R√©gression Logistique, SVM, Random Forest ou Naive Bayes\n",
    "   - Avantages: rapides √† entra√Æner, interpr√©tables\n",
    "   - Inconv√©nients: ne capturent pas la s√©mantique et l'ordre des mots\n",
    "\n",
    "2. Word Embeddings + Classifieurs:\n",
    "   - Utiliser des embeddings pr√©-entra√Æn√©s (Word2Vec, GloVe, FastText) avec des classifieurs ML\n",
    "   - Avantages: capture la s√©mantique des mots, g√®re les mots hors vocabulaire (FastText)\n",
    "   - Inconv√©nients: perd l'information de s√©quence\n",
    "\n",
    "3. R√©seaux de neurones r√©currents (RNN, **LSTM**, GRU):\n",
    "   - Avantages: capture l'information s√©quentielle, efficace pour le texte\n",
    "   - Inconv√©nients: temps d'entra√Ænement plus long, risque de surapprentissage\n",
    "\n",
    "4. Mod√®les transformers (BERT, Sentence Transformers, RoBERTa, DistilBERT):\n",
    "   - Fine-tuning de mod√®les pr√©-entra√Æn√©s sp√©cifiques √† Twitter comme BERTweet\n",
    "   - Avantages: √©tat de l'art en NLP, capture le contexte bidirectionnel\n",
    "   - Inconv√©nients: co√ªteux en ressources, complexe √† d√©ployer\n",
    "\n",
    "5. Approches d'ensemble:\n",
    "   - Combiner plusieurs mod√®les pour obtenir de meilleures performances\n",
    "   - Avantages: souvent meilleures performances, plus robuste\n",
    "   - Inconv√©nients: complexit√© accrue, temps d'inf√©rence plus long\n",
    "\n",
    "Consid√©rations importantes:\n",
    "\n",
    "1. D√©s√©quilibre des classes: utiliser des techniques comme SMOTE, sous-√©chantillonnage, ou pond√©ration des classes\n",
    "2. Validation crois√©e: essentielle pour √©valuer correctement les performances\n",
    "3. M√©triques d'√©valuation: ne pas se limiter √† l'accuracy, utiliser F1-score, pr√©cision, rappel, et AUC-ROC\n",
    "4. Interpr√©tabilit√©: pour certaines applications, privil√©gier des mod√®les interpr√©tables ou utiliser SHAP/LIME\n",
    "5. D√©pendance temporelle: consid√©rer l'√©volution du langage sur Twitter au fil du temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes du mentor :**\n",
    "\n",
    "Voici le texte extrait de cette capture d'√©cran :\n",
    "\n",
    "- Cr√©ation de deux mod√®les de Deep Learning, dont au moins un avec un layer LSTM.\n",
    "- Simulation selon deux techniques de pr√©-traitement (lemmatization, stemming) sur l'un des 2 mod√®les, afin de choisir la technique pour la suite des simulations.\n",
    "- Simulation selon 2 approches de word embedding (parmi Word2VEc, Glove, FastText), entra√Æn√©s avec le jeu de donn√©es ou pr√©-entra√Æn√©s sur au moins un des 2 mod√®les de Deep Learning, afin de choisir l'embedding pour la suite des simulations.\n",
    "- Cr√©ation ensuite d'un mod√®le BERT, il y a 2 approches possibles :\n",
    "  - G√©n√©rer des features (sentence embedding) √† partir d'un TFBertModel (Hugging Face) ou d'un d'un model via le Hub TensorFlow, puis ajouter une ou des couches de classification\n",
    "  - Utiliser directement un mod√®le Hugging Face de type TFBertForSequenceClassification\n",
    "- En option tester USE (Universal Sentence Encoding) pour le feature engineering\n",
    "\n",
    "Probl√®mes et erreurs courants :\n",
    "- Temps de traitement et limitation de ressources en TensorFlow-Keras.\n",
    "- Inspirer des exemples de mod√®les cit√©s en ressources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approches classiques de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag             user                                               text\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = raw_data.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# Fonction de pr√©traitement pour les tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Pr√©traite un tweet en appliquant plusieurs transformations :\n",
    "    - Conversion en minuscules\n",
    "    - Remplacement des URLs, mentions et hashtags par des tokens sp√©ciaux\n",
    "    - Suppression des caract√®res sp√©ciaux\n",
    "    - Tokenisation et lemmatisation\n",
    "    - Suppression des stopwords\n",
    "    \"\"\"\n",
    "    # V√©rifier si le tweet est une cha√Æne de caract√®res\n",
    "    if not isinstance(tweet, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remplacer les URLs par un token sp√©cial\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', tweet)\n",
    "    \n",
    "    # Remplacer les mentions par un token sp√©cial\n",
    "    tweet = re.sub(r'@\\w+', '<MENTION>', tweet)\n",
    "    \n",
    "    # Traiter les hashtags (conserver le # comme token s√©par√© et le mot qui suit)\n",
    "    tweet = re.sub(r'#(\\w+)', r'# \\1', tweet)\n",
    "    \n",
    "    # Supprimer les caract√®res sp√©ciaux et les nombres, mais garder les tokens sp√©ciaux\n",
    "    tweet = re.sub(r'[^\\w\\s<>@#!?]', '', tweet)\n",
    "    \n",
    "    # Tokenisation\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Supprimer les stopwords, mais conserver les n√©gations importantes\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    important_words = {'no', 'not', 'nor', 'neither', 'never', 'nobody', 'none', 'nothing', 'nowhere'}\n",
    "    stop_words = stop_words - important_words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Rejoindre les tokens en une cha√Æne\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def process_in_parallel(df, func, n_jobs=4):\n",
    "    df_split = np.array_split(df, n_jobs)\n",
    "    pool = Pool(n_jobs)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def apply_preprocessing(df_part):\n",
    "    df_part['processed_text'] = df_part['text'].apply(preprocess_tweet)\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr√©traitement des tweets en cours...\n",
      "Pr√©traitement termin√© !\n"
     ]
    }
   ],
   "source": [
    "# Appliquer le pr√©traitement √† tous les tweets\n",
    "print(\"Pr√©traitement des tweets en cours...\")\n",
    "# df['processed_text'] = df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Pr√©paration des donn√©es pour l'entra√Ænement\n",
    "# Convertir les √©tiquettes de sentiment (0 et 4) en binaire (0 et 1)\n",
    "df['target'] = df['target'].map({0: 0, 4: 1})\n",
    "\n",
    "# Pour acc√©l√©rer le d√©veloppement, nous pouvons utiliser un sous-ensemble des donn√©es\n",
    "# Utiliser un √©chantillon √©quilibr√© des donn√©es\n",
    "negative_samples = df[df['target'] == 0].sample(n=50000, random_state=42)\n",
    "positive_samples = df[df['target'] == 1].sample(n=50000, random_state=42)\n",
    "balanced_df = pd.concat([negative_samples, positive_samples])\n",
    "\n",
    "# Utilisation\n",
    "balanced_df = process_in_parallel(balanced_df, apply_preprocessing, n_jobs=8)\n",
    "\n",
    "print(\"Pr√©traitement termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire des features suppl√©mentaires bas√©es sur nos observations\n",
    "def extract_features(tweet):\n",
    "    \"\"\"\n",
    "    Extrait des features suppl√©mentaires d'un tweet\n",
    "    \"\"\"\n",
    "    if not isinstance(tweet, str):\n",
    "        return pd.Series([0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    features = {\n",
    "        'url_count': len(re.findall(r'http[s]?://\\S+', tweet)),\n",
    "        'mention_count': len(re.findall(r'@\\w+', tweet)),\n",
    "        'hashtag_count': len(re.findall(r'#\\w+', tweet)),\n",
    "        'exclamation_count': tweet.count('!'),\n",
    "        'question_count': tweet.count('?'),\n",
    "        'ellipsis_count': len(re.findall(r'\\.{3,}', tweet)),\n",
    "    }\n",
    "    \n",
    "    return pd.Series(features)\n",
    "\n",
    "# Extraire les features suppl√©mentaires\n",
    "print(\"Extraction des features suppl√©mentaires...\")\n",
    "features_df = balanced_df['text'].apply(extract_features)\n",
    "balanced_df = pd.concat([balanced_df, features_df], axis=1)\n",
    "print(\"Extraction termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entra√Ænement: 80000 exemples\n",
      "Taille de l'ensemble de test: 20000 exemples\n",
      "Vectorisation termin√©e !\n"
     ]
    }
   ],
   "source": [
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = balanced_df['processed_text']\n",
    "y = balanced_df['target']\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Taille de l'ensemble d'entra√Ænement: {X_train.shape[0]} exemples\")\n",
    "print(f\"Taille de l'ensemble de test: {X_test.shape[0]} exemples\")\n",
    "\n",
    "# Vectorisation des textes avec TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Cr√©er les vectoriseurs\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "bow_vectorizer = CountVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "# Transformer les textes en vecteurs TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Transformer les textes en vecteurs BoW\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Vectorisation termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
